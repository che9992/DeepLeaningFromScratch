{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss_function <hr/>\n",
    "\n",
    "### What is loss function?\n",
    "Loss function is also called cost function. **the loss function represents the current state as number in the neural network.** then what we have to do is control the weight variables to make the loss value close to zero, to make the loss function\n",
    "Generally, we use the mean square error and the cross entropy error\n",
    "\n",
    "\n",
    "### Mean squared error, MSE\n",
    "The formula for Mean squared error is as follows\n",
    "![mean squared error](./images/mean_squared_error.png)\n",
    "\n",
    "**y** is prediction by neural network, **t** is answer and **k** is dimension of data,\n",
    "we can implement this in Python as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0, 0.05, 0.1, 0, 0.1, 0, 0] # after passing soft max funtion\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ] # one_hot_encoding anwser is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value :  0.09750000000000003\n"
     ]
    }
   ],
   "source": [
    "print('loss value : ', mean_squared_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = [0.1, 0.05, 0.6, 0, 0.05, 0.1, 0, 0.1, 0, 0] # after passing soft max funtion\n",
    "t2 = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ] # one_hot_encoding anwser is 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value :  0.6975\n"
     ]
    }
   ],
   "source": [
    "print('loss value : ', mean_squared_error(np.array(y2), np.array(t2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first is the correct answer 2 and prediction is also 2, the second one is the correct answer 2 and prediction is 7, you can see that the higher the loss value, the more wrong the prediction.\n",
    "\n",
    "next, let's find what is cross entropy error out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy error, CEE\n",
    "The formula for Cross entropy error is as follows\n",
    "![cross_entropy_error](./images/cross_entropy_error.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value :  0.510825457099338\n",
      "loss value :  16.11809565095832\n"
     ]
    }
   ],
   "source": [
    "print('loss value : ', cross_entropy_error(np.array(y), np.array(t)))\n",
    "print('loss value : ', cross_entropy_error(np.array(y2), np.array(t2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batch learning\n",
    "\n",
    "So far we have only thought of the loss function for one piece of data. what are we going to do? if we want a loss function for all of data? below is the formula\n",
    "\n",
    "![cross_entropy_error2](./images/cross_entropy_error2.png)\n",
    "\n",
    "mini-batch learning means train with only a part of the materials. so  the part of the materials are mini-batch\n",
    "\n",
    "so how are we randomly choose data of specified number from all of data?\n",
    "first, we going to make 60000 of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(1,784*60000+1).reshape([60000,784])\n",
    "y = np.arange(1, 60000*10+1).reshape([60000,10])\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(x, y, batch_size):\n",
    "    train_size = x.shape[0]\n",
    "    batch_size = batch_size\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch, y_batch = x[batch_mask], y[batch_mask]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784) (100, 10)\n"
     ]
    }
   ],
   "source": [
    "x_mini, y_mini = mini_batch(x,y,100)\n",
    "print(x_mini.shape,y_mini.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see? we can easily do mini-batch\n",
    "so, how are we get cross entropy error from mini-batch?\n",
    "\n",
    "it is version to use when y value is one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error_mn(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if y isn't one_hot_encoded,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error_mn(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
